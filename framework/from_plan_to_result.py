import json
import os
import re
import sys
import asyncio
import time
from typing import List, Dict

from llm import llm_client
from config import OPENAI_API_KEY, OPENAI_API_BASE, GPT_MODEL
from caller import TaskExecutor

RESULT_PROMPT = """
You are a world-class academic expert. You will answer questions from any academic field.

Instructions:

Your answer must be in the same language as the question. 

The answer must be concise and scholarly, no longer than 300 words.

Analyze the user's question to determine the underlying intent(s). For each distinct intent, provide a separate, clearly labeled point in your answer.

Use the following knowledge base information (api_output) as your primary source. Only use data from the knowledge base that is directly relevant to the question and its intent(s). Do not use unrelated information, even if present in the knowledge base.

If your answer contains any information drawn from api_output (even partially), you must include a citation [e.g., [1], [2]] for that content. If the information is insufficient, you may supplement with your own knowledge, but always prioritize and cite api_output when used.
In the "reference" field, each citation label (e.g., [1], [2]) should map to a full citation (with the appropriate link, as specified above).
In the "reference" field, only use links or IDs that are present in the knowledge base (api_output); do not invent, infer, or fabricate any links or IDs.
In the "reference" field, citations must be numbered sequentially as [1], [2], [3], [4], etc., starting from [1] and increasing by 1 for each new citation. No duplicate citations are allowed in the "reference" field; each cited item should appear only once in the reference list.

If a citation is an ID, determine its type (paper, author, venue, or org) based on the knowledge base (api_output), and generate the corresponding full URL as follows:
- For papers: https://www.aminer.cn/pub/paper_id
- For scholars: https://www.aminer.cn/profile/author_id
- For journals/conferences: https://www.aminer.cn/open/journal/detail/venue_id
- For institutions: https://www.aminer.cn/institution/org_id
Only generate such URLs if the ID exists in the knowledge base (api_output); do not fabricate or infer IDs.

Output only a JSON object with two fields:
- "answer": your academic answer, written in the same language as the question (max 300 words).
- "reference": a dictionary mapping each citation label to its bibliographic citation (as a webpage or ID from the knowledge base).

Example output:
{{
"answer": "Your detailed answer here, with inline citations [1][2] ...",
"reference": {{"[1]": "https://xxx", "[2]": "ID"}}
}}

Here is the knowledge base output (api_output):
{api_output}

Here is the question:
{question}
"""

PLAN_PROMPT = """ä½ æ˜¯ä¸€ä¸ªè§„åˆ’ä¸“å®¶ï¼Œä½ å°†æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œé€‰æ‹©ä¸€ä¸ªæˆ–å¤šä¸ªåˆé€‚çš„APIï¼Œä½¿ç”¨APIæ£€ç´¢åˆ°ç›¸å…³ä¿¡æ¯ï¼Œä»è€Œå¯ä»¥å‡†ç¡®å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚ã€thinkingè¿‡ç¨‹ä¸è¦è¶…è¿‡åå¥è¯ã€‘
   ä½ ç”Ÿæˆçš„æ ¼å¼ç¤ºä¾‹å¦‚ä¸‹ï¼š
    [
        {
            "name": "search_author_id",
            "rely": [],
            "order": 1,
            "params": {"interest": ["Optical communication"]}
        },
        {
            "name": "search_author_detail",
            "rely": ["search_author_id"],
            "order": 2,
            "params": {"ids": []}
        },
    ]
    # è¯´æ˜ï¼šå…¶ä¸­ï¼ŒnameæŒ‡çš„æ˜¯APIçš„åç§°ï¼Œrelyæ˜¯è¾“å…¥å‚æ•°çš„æ¥æºAPIåç§°ï¼Œåªèƒ½ä¸ºapiåç§°ï¼ŒorderæŒ‡çš„æ˜¯æ‰§è¡Œçš„é¡ºåºï¼Œå¦‚1ï¼Œ2ï¼Œ3ï¼›paramsæ˜¯APIçš„å‚æ•°
    # æ³¨æ„ï¼šä½ ä¸éœ€è¦ç”Ÿæˆé¢å¤–çš„ä»»ä½•è§£é‡Šï¼Œåªéœ€è¦ç”Ÿæˆä¸Šé¢è¯´æ˜ç”Ÿæˆjsonå†…å®¹å³å¯ï¼ï¼ˆä¸éœ€è¦å‡ºç°```ã€jsonç­‰å­—çœ¼ï¼‰
    # æ³¨æ„ï¼šå¦‚æœå‚æ•°æ²¡æœ‰å…·ä½“å€¼ï¼Œåˆ™ç”¨""ç©ºå­—ç¬¦ã€æˆ–è€…ç©ºlistï¼Œæˆ–å…¶ä»–ç©ºçš„å€¼
    # æ³¨æ„ï¼štrueå’Œfalseå¿…é¡»é¦–å­—æ¯å°å†™ï¼
    # æ³¨æ„ï¼šæ²¡æœ‰æŒ‡å®šæ’åºæ–¹å¼çš„æƒ…å†µä¸‹ï¼Œéƒ½æŒ‰ç…§citationæ’åºï¼
    # æ³¨æ„ï¼šå¦‚æœæœ‰å¤šä¸ªç›¸åŒåå­—çš„apiï¼Œåˆ™åœ¨nameä¸­åŠ ç¼–å·ï¼Œå¦‚search_paper_id(1)ã€search_paper_id(2)ç­‰
    # æ³¨æ„ï¼šå¦‚æœé—®é¢˜æ²¡æœ‰æŒ‡å®šä¸ºä¸­æ–‡ï¼Œå…³é”®è¯ä¸€å¾‹ä½¿ç”¨è‹±æ–‡å•è¯ï¼

    # æ³¨æ„ï¼šä½ ä¸è¦å±€é™äºä¸Šé¢çš„è§„åˆ’é¡ºåºï¼Œä½ å¯ä»¥æŒ‰ç…§ä½ çš„çŸ¥è¯†ç»™å‡ºä»»ä½•çš„apiè°ƒç”¨é¡ºåºï¼

    # å¯ç”¨çš„APIå¦‚ä¸‹ï¼š
ã€search_paper_idã€‘
    "search_paper_id": {
        "description": "æ ¹æ®æ¡ä»¶æœç´¢è®ºæ–‡ID",
        "parameters": {
            "titles": ["è®ºæ–‡æ ‡é¢˜"], 
            "keywords": ["å…³é”®è¯"], 
            "years": {
                "type": "array",
                "description": "å‘è¡¨å¹´ä»½åˆ—è¡¨, å¹´ä»½ç”¨æ•´æ•°è¡¨ç¤º"
            },
            "is_sci": {
                "type": "boolean",
                "description": "æ˜¯å¦ä¸ºSCIè®ºæ–‡"
            },
            "language": "è®ºæ–‡è¯­è¨€(str), ä½¿ç”¨ISO 639-1æ ‡å‡†å¦‚'en', 'zh'",
            "sort": "æ’åºæ–¹å¼(str)ï¼Œåªèƒ½é€‰æ‹©: year, citation",
            "author": "ä½œè€…å§“å(str)",
            "author_id": "ä½œè€…ID(str)",
            "coauthors": {
                "type": "array",
                "description": "å…±åŒä½œè€…å§“ååˆ—è¡¨ï¼ŒæŒ‡çš„æ˜¯ä¸åŒ…æ‹¬authorå‚æ•°ä½œè€…çš„å…¶ä»–ä½œè€…"
            },
            "org": "æœºæ„æˆ–å­¦æ ¡åç§°(str)",
            "org_id": "æœºæ„æˆ–å­¦æ ¡ID(str)",
            "venues": {
                "type": "array",
                "description": "æœŸåˆŠæˆ–ä¼šè®®åˆ—è¡¨ï¼Œä½¿ç”¨è‹±æ–‡å°å†™ç¼©å†™ï¼Œä¸éœ€è¦å¸¦å¹´ä»½"            
            },
            "venue_ids": ["æœŸåˆŠæˆ–ä¼šè®®ID"], 
            "size": {
                "type": "integer",
                "description": "è¿”å›ç»“æœæ•°é‡ï¼Œå¿…é¡»å°äº100"                   
            }
        },
        "response": {
            "description": "è¿”å›ç»“æœåˆ—è¡¨, æ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ªå­—å…¸(ä»£è¡¨ä¸€ç¯‡è®ºæ–‡)",
            "data": [
                {
                    "paper_id": "è®ºæ–‡ID(str)",
                    "title": "è®ºæ–‡æ ‡é¢˜(str)"
                }
            ]
        }                     
    },

---
ã€search_paper_detailã€‘
    "search_paper_detail":{
        "description": "æ ¹æ®è®ºæ–‡IDåˆ—è¡¨ï¼Œè·å–è®ºæ–‡è¯¦ç»†ä¿¡æ¯",
        "parameters": {
            "paper_ids": ["è®ºæ–‡ID"]
        },
        "response":{
            "description": "è¿”å›ç»“æœåˆ—è¡¨, æ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ªå­—å…¸(ä»£è¡¨ä¸€ç¯‡è®ºæ–‡)",
            "data": [
                {
                    "paper_id": "è®ºæ–‡ID(str)",
                    "title": "è®ºæ–‡æ ‡é¢˜(str)",
                    "abstract": "è®ºæ–‡æ‘˜è¦(str)",
                    "year": "å‘è¡¨å¹´ä»½(float)",
                    "citation": "è¢«å¼•ç”¨æ¬¡æ•°(float)",
                    "keywords": ["å…³é”®è¯"],
                    "authors": [
                        {
                            "author": "ä½œè€…å§“å(str)",
                            "author_id": "ä½œè€…ID(str)",
                            "org": "ä½œè€…æœºæ„(str)",
                            "org_id": "ä½œè€…æœºæ„ID(str)",
                            "email":"ä½œè€…é‚®ç®±(str)"
                        }
                    ],
                    "org": "å‘è¡¨æœºæ„(str)",
                    "org_id": "å‘è¡¨æœºæ„ID(str)",    
                    "venue": "å‘è¡¨çš„æœŸåˆŠæˆ–ä¼šè®®(str)"
                }
            ]
        }
    },

---
ã€search_author_idã€‘
    {
        "type": "function",
        "function":{
            "name": "search_author_id",
            "description": "æ ¹æ®å§“åã€æœºæ„ã€å…´è¶£ã€å›½å®¶ç­‰æ¡ä»¶æœç´¢å­¦è€…",
            "parameters": {
                "type": "object",
                "properties": {
                    "name": {
                        "type": "string",
                        "description": "å­¦è€…å§“å",
                    },
                    "org": {
                        "type": "string",
                        "description": "å­¦è€…æ‰€åœ¨æœºæ„",
                    },
                    "size": {
                        "type": "integer",
                        "description": "è¿”å›çš„å­¦è€…æ•°é‡ï¼Œæœ€å¤§ä¸º1000",
                    },
                    "interest": {
                        "type": "list",
                        "description": "å­¦è€…å…´è¶£ï¼Œæ ¼å¼ä¸º[str,str,...]"
                    },
                    "nation": {
                        "type": "list",
                        "description": "å­¦è€…æ‰€åœ¨å›½å®¶ï¼Œæ ¼å¼ä¸º[str,str,...]
                    },
                    "order": {
                        "type": "string",
                        "description": "æ’åºå­—æ®µå n_citation, n_pubs, h_index"
                    },
                    "asc": {
                        "type": "boolean",
                        "description": "true å‡åº false é™åº"
                    }
                },
                "required": []
            }
        }
    },
---
ã€search_author_detailã€‘æ ¹æ®å­¦è€…IDåˆ—è¡¨è·å–å­¦è€…è¯¦æƒ…
è¾“å…¥ï¼šidsï¼š[str,str, ...]  #idåˆ—è¡¨

---
ã€search_venue_idã€‘æ ¹æ®æœŸåˆŠåç§°æœç´¢æœŸåˆŠIDã€æœŸåˆŠæ ‡å‡†åç§°
è¾“å…¥ï¼š 
    nameï¼šstr  #æœŸåˆŠåç§°
    categoryï¼šstr  #å­¦ç§‘åˆ†ç±»åç§°
    category_source:string # ä½¿ç”¨æ•°å­—å½¢å¼å­—ç¬¦ä¸²ï¼Œä¸åŒçš„æ•°å­—çš„å«ä¹‰å¦‚ä¸‹ï¼š 0: "SJR", 1: "WOS", 2: "GB09", 3: "CCF", 4: "CSCD", 5: "CCJ", 6: "ARXIV", 7: "CJCR", 8: "JCR", 9: "SCI"
    quartileï¼šstr  #æœŸåˆŠåˆ†åŒºæœç´¢ å¦‚"1åŒº", "A", "Q1"
    keywordsï¼šlist  #æœŸåˆŠå…³é”®è¯åˆ—è¡¨
    sizeï¼šnumber  #è¿”å›çš„æœŸåˆŠæ•°é‡
è¾“å‡ºï¼šæœŸåˆŠid

---
ã€search_venue_detailã€‘æ ¹æ®æœŸåˆŠIDè·å–æœŸåˆŠè¯¦æƒ…
è¾“å…¥ï¼š
    idsï¼š[str,str, ...]  #idåˆ—è¡¨
è¾“å‡ºï¼š
alias	array	åˆ«å
category_id	string	å­¦ç§‘é¢†åŸŸid
classes	array	æ•°æ®æº
id	string	id
issn	string	ISSN
lower_alias	array	åˆ«åï¼ˆå°å†™ï¼‰
name	string	å§“å
name_en	string	æœºæ„è‹±æ–‡åç§°
name_zh	string	ä¸­æ–‡å
num	float	ä¼˜å…ˆæƒå·
quartile	string	åˆ†åŒº
source_quartiles	array	æºåˆ†åŒº
total	float	æ€»æ•°
type	string	åˆ†ç±»ä½“ç³»
url	string	æ¥æºurl    

---
ã€search_org_idã€‘æ ¹æ®åç§°å…³é”®è¯æœç´¢æœºæ„IDã€åç§°
è¾“å…¥ï¼š
    orgsï¼š[str,str,...]  #æœºæ„åç§°åˆ—è¡¨
è¾“å‡ºï¼š
    æœºæ„idåˆ—è¡¨

---
ã€search_org_detailã€‘é€šè¿‡æœºæ„IDè·å–æœºæ„è¯¦æƒ…
è¾“å…¥ï¼š  
    idsï¼š[str,str, ...]  #idåˆ—è¡¨
è¾“å‡ºï¼š
acronyms	array	
aliases	array	æœºæ„åˆ«å
coordinate	array	
details	array	æœºæ„è¯¦æƒ…
error	array	
established	int	æˆç«‹æ—¶é—´
external_ids	array	
geographic_id	string	åœ°ç†id
id	string	æœºæ„id
image	string	å›¾ç‰‡
introduction	string	ç®€ä»‹
language	string	è¯­è¨€
latitude	float	çº¬åº¦
longitude	float	ç»åº¦
name	string	æœºæ„åç§°
name_en	string	æœºæ„è‹±æ–‡åç§°
name_zh	string	ä¸­æ–‡å
relationships	array	
src	string	æ•°æ®æº
total	int	è¿”å›æ•°æ®æ¡æ•°
type	string	æœºæ„ç±»å‹

ã€search_paper_id_gsã€‘å€ŸåŠ©è°·æ­Œå­¦æœ¯æœç´¢å¾—åˆ°è®ºæ–‡id
è¾“å…¥ï¼š  
    query: "str" # ç”¨æˆ·é—®é¢˜
"""


def _repair_json_trailing_comma(s: str) -> str:
    """ä¿®å¤å¸¸è§ JSON è¯­æ³•é”™è¯¯ï¼šæœ«å°¾å¤šä½™é€—å·ã€å¯¹è±¡/æ•°ç»„é—´ç¼ºå°‘é€—å·ã€‚"""
    s = re.sub(r",\s*\]", "]", s)
    s = re.sub(r",\s*}", "}", s)
    # å¯¹è±¡æˆ–æ•°ç»„ä¹‹é—´ç¼ºå°‘é€—å·ï¼ˆå¦‚ }  { æˆ– ]  [ï¼‰ï¼Œå¸¸è§äºæ¨¡å‹è¾“å‡º
    s = re.sub(r"\}\s*\n\s*\{", "},\n{", s)
    s = re.sub(r"\]\s*\n\s*\[", "],\n[", s)
    return s


def _parse_plan_response(plan):
    """å°† LLM è¿”å›çš„è§„åˆ’è§£æä¸º listï¼ˆAPI è§„åˆ’æ•°ç»„ï¼‰ã€‚å¤„ç†ç©ºå­—ç¬¦ä¸²ã€markdownã€Extra dataã€è¯­æ³•é”™è¯¯ç­‰ã€‚"""
    if isinstance(plan, list) and plan:
        return plan
    if isinstance(plan, dict):
        return [plan] if plan else []
    s = (plan or "").strip()
    if not s:
        raise ValueError("æ¨¡å‹è¿”å›çš„è§„åˆ’ä¸ºç©º")
    # å»æ‰ ```json ... ``` æˆ– ``` ... ```
    if s.startswith("```"):
        lines = s.split("\n")
        if lines[0].startswith("```"):
            lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        s = "\n".join(lines)

    def try_parse(raw: str):
        try:
            return json.loads(raw)
        except json.JSONDecodeError:
            return None

    # å…ˆå°è¯•ç›´æ¥è§£æ
    obj = try_parse(s)
    if obj is not None:
        return obj if isinstance(obj, list) else [obj]

    # å°è¯•ä¿®å¤æœ«å°¾é€—å·åè§£æï¼ˆè§£å†³ "Expecting ',' delimiter" ç­‰è¯­æ³•é—®é¢˜ï¼Œå¦‚ id 1960ï¼‰
    repaired = _repair_json_trailing_comma(s)
    obj = try_parse(repaired)
    if obj is not None:
        return obj if isinstance(obj, list) else [obj]

    # å¤„ç† "Extra data"ï¼šå‰é¢æ˜¯åˆæ³• JSON åé¢æœ‰å¤šä½™å†…å®¹ï¼ˆå¦‚ id 915 è¿”å› "[]" + çœŸå®è§„åˆ’ï¼‰
    decoder = json.JSONDecoder()
    idx = 0
    while idx < len(s):
        s_rest = s[idx:].lstrip()
        if not s_rest:
            break
        try:
            obj, end = decoder.raw_decode(s_rest)
            idx += len(s_rest) - len(s_rest.lstrip()) + end
            # è‹¥è§£æåˆ°ç©ºæ•°ç»„ []ï¼Œå¯èƒ½æ˜¯æ¨¡å‹å…ˆè¾“å‡ºäº† [] å†è¾“å‡ºçœŸå®è§„åˆ’ï¼Œç»§ç»­å¾€åæ‰¾éç©º list
            if isinstance(obj, list) and len(obj) > 0:
                return obj
            if isinstance(obj, dict) and obj:
                return [obj]
        except json.JSONDecodeError:
            break

    # æœ€åå†è¯•ä¸€æ¬¡æ•´æ®µä¿®å¤åè§£æ
    obj = try_parse(_repair_json_trailing_comma(s))
    if obj is not None:
        return obj if isinstance(obj, list) else [obj]

    preview = (s[:200] + "â€¦") if len(s) > 200 else s
    raise ValueError(f"è§„åˆ’ä¸æ˜¯åˆæ³• JSONï¼Œè¿”å›å†…å®¹é¢„è§ˆ: {preview!r}")


# å•ä¸ªé—®é¢˜çš„å¤„ç†é€»è¾‘ï¼ˆå¸¦é‡è¯•ï¼‰
async def process_single_question(item: Dict, idx: int) -> Dict:
    question = item.get("question", "").strip()
    question_id = item.get("id", idx + 1)
    if not question:
        return None

    max_retries = 3
    for attempt in range(max_retries):
        try:
            print(f"\n===== å¤„ç†é—®é¢˜ {idx+1}ï¼ˆå°è¯• {attempt+1}/{max_retries}ï¼‰=====")
            print("é—®é¢˜å†…å®¹ï¼š", question)

            plan_prompt = PLAN_PROMPT + "ç”¨æˆ·é—®é¢˜å¦‚ä¸‹ï¼š" + question
            # ä½¿ç”¨ GPTï¼ˆäº‘é›¾ APIï¼‰
            plan = llm_client(
                prompt=plan_prompt,
                query=question,
                model=GPT_MODEL,
                api_key=OPENAI_API_KEY,
                base_url=OPENAI_API_BASE,
            )
            print("ã€è§„åˆ’ã€‘\n", plan)

            plan_obj = _parse_plan_response(plan)
            executor = TaskExecutor(plan_obj)
            result = await executor.run()
            print("ã€æ‰§è¡Œç»“æœã€‘\n", result)

            summary_prompt = RESULT_PROMPT.format(api_output=result, question=question)
            summary = llm_client(
                prompt=summary_prompt,
                query=question,
                model=GPT_MODEL,
                api_key=OPENAI_API_KEY,
                base_url=OPENAI_API_BASE,
            )
            
            print("ã€æ€»ç»“ã€‘\n", summary)

            return {
                "id": question_id,
                "question": question,
                "plan": plan,
                "execution_result": result,
                "summary": summary
            }

        except Exception as e:
            error_msg = str(e)
            print(f"âŒ é—®é¢˜ {idx+1} å¤„ç†å¤±è´¥ï¼ˆå°è¯• {attempt+1}/{max_retries}ï¼‰ï¼š{error_msg}")
            
            is_concurrency_error = any(keyword in error_msg.lower() for keyword in [
                "å¹¶å‘", "concurrency", "rate limit", "429", "too many requests", 
                "è¯·æ±‚è¿‡å¤š", "è¶…å‡ºé™åˆ¶", "exceeded", "limit"
            ])
            
            if is_concurrency_error:
                print(f"âš ï¸ æ£€æµ‹åˆ° API å¹¶å‘é”™è¯¯ï¼Œè·³è¿‡é‡è¯•")
                return {
                    "id": question_id,
                    "question": question,
                    "error": error_msg,
                    "error_type": "concurrency_error"
                }
            
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt
                print(f"â³ æŒ‡æ•°é€€é¿ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
                continue
            else:
                return {
                    "id": question_id,
                    "question": question,
                    "error": error_msg
                }


async def process_question_batch(batch: List[Dict], batch_idx: int, start_idx: int, output_dir: str, output_file: str) -> List[Dict]:
    results = []
    
    if os.path.exists(output_file):
        with open(output_file, 'r', encoding='utf-8') as f:
            try:
                results = json.load(f)
            except json.JSONDecodeError:
                results = []
    
    for i, item in enumerate(batch):
        result = await process_single_question(item, start_idx + i)
        if result is not None:
            results.append(result)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"âœ… é—®é¢˜ {start_idx + i + 1} å¤„ç†å®Œæˆå¹¶å·²ä¿å­˜åˆ° {output_file}")
        
        if i < len(batch) - 1:
            print(f"â³ ç­‰å¾… 5 ç§’åå¤„ç†ä¸‹ä¸€ä¸ªé—®é¢˜...")
            time.sleep(5)

    batch_output_file = os.path.join(output_dir, f"output_batch_{batch_idx}.json")
    with open(batch_output_file, 'w', encoding='utf-8') as f:
        json.dump(results[-(len(batch)):], f, ensure_ascii=False, indent=2)
    print(f"âœ… æ‰¹æ¬¡ {batch_idx} å®Œæˆï¼Œç»“æœä¿å­˜è‡³ {batch_output_file}")

    return results


def process_questions(input_file: str, output_file: str, batch_size: int = 5):
    output_dir = "glm-batch_outputs"
    os.makedirs(output_dir, exist_ok=True)

    with open(input_file, 'r', encoding='utf-8') as f:
        questions = json.load(f)

    all_results = []
    total_batches = (len(questions) + batch_size - 1) // batch_size

    for batch_idx in range(total_batches):
        start = batch_idx * batch_size
        end = min(start + batch_size, len(questions))
        batch = questions[start:end]
        print(f"\n===== å¼€å§‹å¤„ç†ç¬¬ {batch_idx} æ‰¹ï¼Œå…± {len(batch)} ä¸ªé—®é¢˜ =====")

        batch_results = asyncio.run(process_question_batch(batch, batch_idx, start_idx=start, output_dir=output_dir, output_file=output_file))
        all_results.extend(batch_results)
        
        if batch_idx < total_batches - 1:
            print(f"â³ ç­‰å¾… 10 ç§’åå¤„ç†ä¸‹ä¸€æ‰¹æ¬¡...")
            time.sleep(10)

    print(f"\nğŸ‰ æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œæœ€ç»ˆç»“æœå·²ä¿å­˜è‡³ {output_file}")

    incorrect_questions = [{"question": r["question"]} for r in all_results if "error" in r]
    incorrect_file = "glm-incorrect.json"
    with open(incorrect_file, 'w', encoding='utf-8') as f:
        json.dump(incorrect_questions, f, ensure_ascii=False, indent=2)
    print(f"âŒ å¤±è´¥çš„é—®é¢˜ï¼ˆå…± {len(incorrect_questions)} ä¸ªï¼‰å·²ä¿å­˜è‡³ {incorrect_file}")

    concurrency_errors = [r for r in all_results if r.get("error_type") == "concurrency_error"]
    if concurrency_errors:
        concurrency_file = "glm-concurrency-errors.json"
        concurrency_data = {
            "total_concurrency_errors": len(concurrency_errors),
            "errors": [{"id": r["id"], "question": r["question"], "error": r["error"]} for r in concurrency_errors]
        }
        with open(concurrency_file, 'w', encoding='utf-8') as f:
            json.dump(concurrency_data, f, ensure_ascii=False, indent=2)
        print(f"âš ï¸ API å¹¶å‘é”™è¯¯ï¼ˆå…± {len(concurrency_errors)} ä¸ªï¼‰å·²ä¿å­˜è‡³ {concurrency_file}")
    else:
        print(f"âœ… æœªæ£€æµ‹åˆ° API å¹¶å‘é”™è¯¯")


if __name__ == "__main__":
    process_questions("reactquestion.json", "gpt-5.2-reactquestion-output.json")
